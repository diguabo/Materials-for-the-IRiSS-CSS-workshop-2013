{
 "metadata": {
  "name": "cleaningtext.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Text cleaning\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Removing HTML entities and tags\n",
      "\n",
      "Recall that HTML entities are an artifact from the pre-Unicode era.  Browsers know to render HTML entities a certain way on the page, but we don't need them anymore. \n",
      "\n",
      "Here's some code that will do this for you (function courtesy of the author of lxml)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re, htmlentitydefs\n",
      "\n",
      "##\n",
      "# Removes HTML or XML character references and entities from a text string.\n",
      "#\n",
      "# @param text The HTML (or XML) source text.\n",
      "# @return The plain text, as a Unicode string, if necessary.\n",
      "# AUTHOR: Fredrik Lundh\n",
      "\n",
      "def unescape(text):\n",
      "    def fixup(m):\n",
      "        text = m.group(0)\n",
      "        if text[:2] == \"&#\":\n",
      "            # character reference\n",
      "            try:\n",
      "                if text[:3] == \"&#x\":\n",
      "                    return unichr(int(text[3:-1], 16))\n",
      "                else:\n",
      "                    return unichr(int(text[2:-1]))\n",
      "            except ValueError:\n",
      "                pass\n",
      "        else:\n",
      "            # named entity\n",
      "            try:\n",
      "                text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])\n",
      "            except KeyError:\n",
      "                pass\n",
      "        return text # leave as is\n",
      "    return re.sub(\"&#?\\w+;\", fixup, text)\n",
      "\n",
      "test_string =\"<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the &quot;Chicken Soup for the Soul&quot; series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)\"\n",
      "\n",
      "print test_string\n",
      "print unescape(test_string)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the &quot;Chicken Soup for the Soul&quot; series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)\n",
        "<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the \"Chicken Soup for the Soul\" series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "nltk.clean_html(unescape(test_string.decode('utf8'))) #notice that it returns unicode!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "u'While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don\\'t like the \"Chicken Soup for the Soul\" series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Remember how we made a list of review texts in Text 1?\n",
      "\n",
      "### Exercise 1\n",
      "Create a new list of body texts called `clean_reviews` that are pruned of all HTML entities and tags."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "import nltk\n",
      "os.chdir('/Users/rweiss/Dropbox/presentations/IRiSS2013/text1/fileformats/')\n",
      "\n",
      "with open('amazon/sociology_2010.csv', 'rb') as csvfile:\n",
      "    amazon_reader = csv.DictReader(csvfile, delimiter=',')\n",
      "    amazon_reviews = [row['review_text'] for row in amazon_reader]\n",
      "\n",
      "print len(amazon_reviews)\n",
      "\n",
      "clean_reviews = []\n",
      "for review in amazon_reviews:\n",
      "    clean_reviews.append(nltk.clean_html(unescape(review.decode('utf8'))))\n",
      "\n",
      "print clean_reviews[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "381\n",
        "This book was given to me by my best friend after finishing college. I will always treasure this thoughtful and special gift. _Girlfriends_ is a collection of stories that explore and celebrate female friendship through the eyes, ears, and hearts of everyday women. Some of the women were friends for a lifetime, others for a short time. However, all understood and/or demonstrated the meaning of \"true friendship.\" For example, the stories included everything from the thankful musings of a once-ill woman about the extraordinaty kindness of her girlfriends to a giggly account of how two eerily-simiar best friends met as assigned roomates their first day of college. (The latter tale struck very close to home in a wonderfully spooky way.) While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the \"Chicken Soup for the Soul\" series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.) Rather, I appreciated the \"real\" tone of the stories, as they read like good conversation shared over a nice pot of Hazlenut coffee. Some readers have commented on the book's simple language and lack of depth. I don't think the goal here was to explore the psychology of friendship, rather I think it was intended to be a simple and beautiful celebration meant to be enjoyed by \"Girlfriends\" everywhere. Enjoy!\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tokenizing your text into words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk import word_tokenize\n",
      "\n",
      "tokenized_docs = []\n",
      "for doc in clean_reviews:\n",
      "    term_vector = word_tokenize(doc)\n",
      "    tokenized_docs.append(term_vector)\n",
      "            \n",
      "print tokenized_docs[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'This', u'book', u'was', u'given', u'to', u'me', u'by', u'my', u'best', u'friend', u'after', u'finishing', u'college.', u'I', u'will', u'always', u'treasure', u'this', u'thoughtful', u'and', u'special', u'gift.', u'_Girlfriends_', u'is', u'a', u'collection', u'of', u'stories', u'that', u'explore', u'and', u'celebrate', u'female', u'friendship', u'through', u'the', u'eyes', u',', u'ears', u',', u'and', u'hearts', u'of', u'everyday', u'women.', u'Some', u'of', u'the', u'women', u'were', u'friends', u'for', u'a', u'lifetime', u',', u'others', u'for', u'a', u'short', u'time.', u'However', u',', u'all', u'understood', u'and/or', u'demonstrated', u'the', u'meaning', u'of', u'``', u'true', u'friendship.', u\"''\", u'For', u'example', u',', u'the', u'stories', u'included', u'everything', u'from', u'the', u'thankful', u'musings', u'of', u'a', u'once-ill', u'woman', u'about', u'the', u'extraordinaty', u'kindness', u'of', u'her', u'girlfriends', u'to', u'a', u'giggly', u'account', u'of', u'how', u'two', u'eerily-simiar', u'best', u'friends', u'met', u'as', u'assigned', u'roomates', u'their', u'first', u'day', u'of', u'college.', u'(', u'The', u'latter', u'tale', u'struck', u'very', u'close', u'to', u'home', u'in', u'a', u'wonderfully', u'spooky', u'way.', u')', u'While', u'many', u'of', u'the', u'stories', u'tugged', u'at', u'the', u'heartstrings', u',', u'I', u'never', u'felt', u'manipulated', u'by', u'the', u'authors.', u'(', u'Note', u':', u'Part', u'of', u'the', u'reason', u'why', u'I', u'do', u\"n't\", u'like', u'the', u'``', u'Chicken', u'Soup', u'for', u'the', u'Soul', u\"''\", u'series', u'is', u'that', u'I', u'feel', u'that', u'the', u'authors', u'are', u'just', u'dying', u'to', u'make', u'the', u'reader', u'clutch', u'for', u'the', u'box', u'of', u'tissues.', u')', u'Rather', u',', u'I', u'appreciated', u'the', u'``', u'real', u\"''\", u'tone', u'of', u'the', u'stories', u',', u'as', u'they', u'read', u'like', u'good', u'conversation', u'shared', u'over', u'a', u'nice', u'pot', u'of', u'Hazlenut', u'coffee.', u'Some', u'readers', u'have', u'commented', u'on', u'the', u'book', u\"'s\", u'simple', u'language', u'and', u'lack', u'of', u'depth.', u'I', u'do', u\"n't\", u'think', u'the', u'goal', u'here', u'was', u'to', u'explore', u'the', u'psychology', u'of', u'friendship', u',', u'rather', u'I', u'think', u'it', u'was', u'intended', u'to', u'be', u'a', u'simple', u'and', u'beautiful', u'celebration', u'meant', u'to', u'be', u'enjoyed', u'by', u'``', u'Girlfriends', u\"''\", u'everywhere.', u'Enjoy', u'!']\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Removing punctuation\n",
      "\n",
      "Punctuation can help with tokenizers, but once you've done that, there's no reason to keep it around.  There are tons of ways to remove punctuation.  Since we have already learned regex, how would we do this?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import string\n",
      "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
      "\n",
      "tokenized_docs_no_punctuation = []\n",
      "\n",
      "for review in tokenized_docs:\n",
      "    \n",
      "    new_review = []\n",
      "    for token in review: \n",
      "        new_token = regex.sub(u'', token)\n",
      "        if new_token == u'':\n",
      "            pass\n",
      "        else:\n",
      "            new_review.append(new_token)\n",
      "    \n",
      "    tokenized_docs_no_punctuation.append(new_review)\n",
      "    \n",
      "print tokenized_docs_no_punctuation[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'This', u'book', u'was', u'given', u'to', u'me', u'by', u'my', u'best', u'friend', u'after', u'finishing', u'college', u'I', u'will', u'always', u'treasure', u'this', u'thoughtful', u'and', u'special', u'gift', u'Girlfriends', u'is', u'a', u'collection', u'of', u'stories', u'that', u'explore', u'and', u'celebrate', u'female', u'friendship', u'through', u'the', u'eyes', u'ears', u'and', u'hearts', u'of', u'everyday', u'women', u'Some', u'of', u'the', u'women', u'were', u'friends', u'for', u'a', u'lifetime', u'others', u'for', u'a', u'short', u'time', u'However', u'all', u'understood', u'andor', u'demonstrated', u'the', u'meaning', u'of', u'true', u'friendship', u'For', u'example', u'the', u'stories', u'included', u'everything', u'from', u'the', u'thankful', u'musings', u'of', u'a', u'onceill', u'woman', u'about', u'the', u'extraordinaty', u'kindness', u'of', u'her', u'girlfriends', u'to', u'a', u'giggly', u'account', u'of', u'how', u'two', u'eerilysimiar', u'best', u'friends', u'met', u'as', u'assigned', u'roomates', u'their', u'first', u'day', u'of', u'college', u'The', u'latter', u'tale', u'struck', u'very', u'close', u'to', u'home', u'in', u'a', u'wonderfully', u'spooky', u'way', u'While', u'many', u'of', u'the', u'stories', u'tugged', u'at', u'the', u'heartstrings', u'I', u'never', u'felt', u'manipulated', u'by', u'the', u'authors', u'Note', u'Part', u'of', u'the', u'reason', u'why', u'I', u'do', u'nt', u'like', u'the', u'Chicken', u'Soup', u'for', u'the', u'Soul', u'series', u'is', u'that', u'I', u'feel', u'that', u'the', u'authors', u'are', u'just', u'dying', u'to', u'make', u'the', u'reader', u'clutch', u'for', u'the', u'box', u'of', u'tissues', u'Rather', u'I', u'appreciated', u'the', u'real', u'tone', u'of', u'the', u'stories', u'as', u'they', u'read', u'like', u'good', u'conversation', u'shared', u'over', u'a', u'nice', u'pot', u'of', u'Hazlenut', u'coffee', u'Some', u'readers', u'have', u'commented', u'on', u'the', u'book', u's', u'simple', u'language', u'and', u'lack', u'of', u'depth', u'I', u'do', u'nt', u'think', u'the', u'goal', u'here', u'was', u'to', u'explore', u'the', u'psychology', u'of', u'friendship', u'rather', u'I', u'think', u'it', u'was', u'intended', u'to', u'be', u'a', u'simple', u'and', u'beautiful', u'celebration', u'meant', u'to', u'be', u'enjoyed', u'by', u'Girlfriends', u'everywhere', u'Enjoy']\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cleaning text of stopwords\n",
      "\n",
      "There are some really basic words that just don't matter.  NLTK comes with a list of them for many languages."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "\n",
      "tokenized_docs_no_stopwords = []\n",
      "for doc in tokenized_docs_no_punctuation:\n",
      "    new_term_vector = []\n",
      "    for word in doc:\n",
      "        if word in stopwords.words('english'):\n",
      "            pass\n",
      "        else:\n",
      "            new_term_vector.append(word)\n",
      "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
      "            \n",
      "print tokenized_docs_no_stopwords[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'This', u'book', u'given', u'best', u'friend', u'finishing', u'college', u'I', u'always', u'treasure', u'thoughtful', u'special', u'gift', u'Girlfriends', u'collection', u'stories', u'explore', u'celebrate', u'female', u'friendship', u'eyes', u'ears', u'hearts', u'everyday', u'women', u'Some', u'women', u'friends', u'lifetime', u'others', u'short', u'time', u'However', u'understood', u'andor', u'demonstrated', u'meaning', u'true', u'friendship', u'For', u'example', u'stories', u'included', u'everything', u'thankful', u'musings', u'onceill', u'woman', u'extraordinaty', u'kindness', u'girlfriends', u'giggly', u'account', u'two', u'eerilysimiar', u'best', u'friends', u'met', u'assigned', u'roomates', u'first', u'day', u'college', u'The', u'latter', u'tale', u'struck', u'close', u'home', u'wonderfully', u'spooky', u'way', u'While', u'many', u'stories', u'tugged', u'heartstrings', u'I', u'never', u'felt', u'manipulated', u'authors', u'Note', u'Part', u'reason', u'I', u'nt', u'like', u'Chicken', u'Soup', u'Soul', u'series', u'I', u'feel', u'authors', u'dying', u'make', u'reader', u'clutch', u'box', u'tissues', u'Rather', u'I', u'appreciated', u'real', u'tone', u'stories', u'read', u'like', u'good', u'conversation', u'shared', u'nice', u'pot', u'Hazlenut', u'coffee', u'Some', u'readers', u'commented', u'book', u'simple', u'language', u'lack', u'depth', u'I', u'nt', u'think', u'goal', u'explore', u'psychology', u'friendship', u'rather', u'I', u'think', u'intended', u'simple', u'beautiful', u'celebration', u'meant', u'enjoyed', u'Girlfriends', u'everywhere', u'Enjoy']\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Stemming and Lemmatizing\n",
      "\n",
      "If you have taken linguistics, you may be familiar with morphology.  This is the belief that words have a basic root structure.  If you want to get to the basic term meaning of the word, you can try applying a stemmer or lemmatizer.  Here are three very popular methods ready to go right out of the NLTK box.  It's up to you to see which one you want to use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "\n",
      "porter = PorterStemmer()\n",
      "snowball = SnowballStemmer('english')\n",
      "wordnet = WordNetLemmatizer()\n",
      "\n",
      "preprocessed_docs = []\n",
      "\n",
      "for doc in tokenized_docs_no_stopwords:\n",
      "    final_doc = []\n",
      "    for word in doc:\n",
      "        final_doc.append(porter.stem(word))\n",
      "        #final_doc.append(snowball.stem(word))\n",
      "        #final_doc.append(wordnet.lemmatize(word)) #note that lemmatize() can also takes part of speech as an argument!\n",
      "    preprocessed_docs.append(final_doc)\n",
      "\n",
      "print preprocessed_docs[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Thi', u'book', u'given', u'best', u'friend', u'finish', u'colleg', u'I', u'alway', u'treasur', u'thought', u'special', u'gift', u'Girlfriend', u'collect', u'stori', u'explor', u'celebr', u'femal', u'friendship', u'eye', u'ear', u'heart', u'everyday', u'women', u'Some', u'women', u'friend', u'lifetim', u'other', u'short', u'time', u'Howev', u'understood', u'andor', u'demonstr', u'mean', u'true', u'friendship', u'For', u'exampl', u'stori', u'includ', u'everyth', u'thank', u'muse', u'onceil', u'woman', u'extraordinati', u'kind', u'girlfriend', u'giggli', u'account', u'two', u'eerilysimiar', u'best', u'friend', u'met', u'assign', u'roomat', u'first', u'day', u'colleg', u'The', u'latter', u'tale', u'struck', u'close', u'home', u'wonder', u'spooki', u'way', u'While', u'mani', u'stori', u'tug', u'heartstr', u'I', u'never', u'felt', u'manipul', u'author', u'Note', u'Part', u'reason', u'I', u'nt', u'like', u'Chicken', u'Soup', u'Soul', u'seri', u'I', u'feel', u'author', u'die', u'make', u'reader', u'clutch', u'box', u'tissu', u'Rather', u'I', u'appreci', u'real', u'tone', u'stori', u'read', u'like', u'good', u'convers', u'share', u'nice', u'pot', u'Hazlenut', u'coffe', u'Some', u'reader', u'comment', u'book', u'simpl', u'languag', u'lack', u'depth', u'I', u'nt', u'think', u'goal', u'explor', u'psycholog', u'friendship', u'rather', u'I', u'think', u'intend', u'simpl', u'beauti', u'celebr', u'meant', u'enjoy', u'Girlfriend', u'everywher', u'Enjoy']\n"
       ]
      }
     ],
     "prompt_number": 53
    }
   ],
   "metadata": {}
  }
 ]
}